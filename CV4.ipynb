{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d73c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is the concept of cyclical momentum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b339ae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cyclic learning rates (and cyclic momentum, which usually goes hand-in-hand) is a learning rate scheduling technique for (1) faster training of a network and (2) a finer understanding of the optimal learning rate. Cyclic learning rates have an effect on the model training process known somewhat fancifully as \"superconvergence\".\n",
    "\n",
    "To apply cyclic learning rate and cyclic momentum to a run, begin by specifying a minimum and maximum learning rate and a minimum and maximum momentum. Over the course of a training run, the learning rate will be inversely scaled from its minimum to its maximum value and then back again, while the inverse will occur with the momentum. At the very end of training the learning rate will be reduced even further, an order of magnitude or two below the minimum learning rate, in order to squeeze out the last bit of convergence. image.png The maximum should be the value picked with a learning rate finder procedure, and the minimum value can be ten times lower.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3240a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What callback keeps track of hyperparameter values (along with other data) during training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2081e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights (in a checkpoint file) at some interval, so the model or weights can be loaded later to continue the training from the state saved.\n",
    "\n",
    "A few options this callback provides include:\n",
    "\n",
    "Whether to only keep the model that has achieved the \"best performance\" so far, or whether to save the model at the end of every epoch regardless of performance.\n",
    "Definition of 'best'; which quantity to monitor and whether it should be maximized or minimized.\n",
    "The frequency it should save at. Currently, the callback supports saving at the end of every epoch, or after a fixed number of training batches.\n",
    "Whether only weights are saved, or the whole model is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1f71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. In the color dim plot, what does one column of pixels represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d36a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A pixel is the smallest block of color in a digital picture. The term is also used for the smallest block of color on your computer monitor. one column of pixel (short for picture element) is a small block that represents the amount of gray intensity to be displayed for that particular portion of the image. For most images, pixel values are integers that range from 0 (black) to 255 (white)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. In color dim, what does \"poor teaching\" look like? What is the reason for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c1739",
   "metadata": {},
   "outputs": [],
   "source": [
    "The color images needs to be mapped between two-dimensional pixel positions to three-dimensional color space. Therefore, the color images are in five dimensional Euclidian space, where every pixel can be represented as (x, y, r, g, b) and the fractal dimension varies within the range between 2 to 5.\n",
    "\n",
    "Chroma is the least uniform dimension of color space, because not all colors can achieve the same level of chroma, which leads to \"poor teaching\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6832040d",
   "metadata": {},
   "outputs": [],
   "source": [
    ". Does a batch normalization layer have any trainable parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af7864",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, batch normalization have trainable parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f77f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. In batch normalization during preparation, what statistics are used to normalize? What about during the validation process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch normalization adds two trainable parameters to each layer, so the normalized output is multiplied by a “standard deviation” parameter (gamma) and add a “mean” parameter (beta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71d6a03",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "7. Why do batch normalization layers help models generalize better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cf6c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch normalization is a technique to standardize the inputs to a network, applied to ether the activations of a prior layer or inputs directly. Batch normalization accelerates training, in some cases by halving the epochs or better, and provides some regularization, reducing generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437cb9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "8.Explain between MAX POOLING and AVERAGE POOLING is number eight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc09a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Max pooling is a pooling operation that selects the maximum element from the region of the feature map covered by the filter. Thus, the output after max-pooling layer would be a feature map containing the most prominent features of the previous feature map. Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network. image.png\n",
    "\n",
    "Average pooling computes the average of the elements present in the region of feature map covered by the filter. Thus, while max pooling gives the most prominent feature in a particular patch of the feature map, average pooling gives the average of features present in a patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. What is the purpose of the POOLING LAYER?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89398cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network.\n",
    "\n",
    "The pooling layer summarises the features present in a region of the feature map generated by a convolution layer. So, further operations are performed on summarised features instead of precisely positioned features generated by the convolution layer. This makes the model more robust to variations in the position of the features in the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce2c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Why do we end up with Completely CONNECTED LAYERS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b447105",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fully Connected layers in a neural networks are those layers where all the inputs from one layer are connected to every activation unit of the next layer. In most popular machine learning models, the last few layers are full connected layers which compiles the data extracted by previous layers to form the final output.\n",
    "\n",
    "The output from the convolutional layers represents high-level features in the data. While that output could be flattened and connected to the output layer, adding a fully-connected layer is a (usually) cheap way of learning non-linear combinations of these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d7ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. What do you mean by PARAMETERS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a078edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "The parameters of a neural network are typically the weights of the connections. In this case, these parameters are learned during the training stage. So, the algorithm itself (and the input data) tunes these parameters. The hyper parameters are typically the learning rate, the batch size or the number of epochs.\n",
    "\n",
    "Model Parameters are properties of training data that will learn during the learning process, in the case of deep learning is weight and bias. Parameter is often used as a measure of how well a model is performing.\n",
    "\n",
    "An algorithm parameter specification is a transparent representation of the sets of parameters used with an algorithm. A transparent representation of a set of parameters means that you can access each parameter value in the set individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae620cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "12. What formulas are used to measure these PARAMETERS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d260285",
   "metadata": {},
   "outputs": [],
   "source": [
    "he parameters of a neural network are typically the weights of the connections. In this case, these parameters are learned during the training stage. So, the algorithm itself (and the input data) tunes these parameters.\n",
    "\n",
    "The hyper parameters are typically the learning rate, the batch size or the number of epochs. The are so called \"hyper\" because they influence how your parameters will be learned. You optimize these hyper parameters as you want (depends on your possibilities): grid search, random search, by hand, using visualisations... The validation stage help you to both know if your parameters have been learned enough and know if your hyper parameters are good.\n",
    "\n",
    "If you want to know more about hyper parameters and parameters in general in machine learning, look for \"deep learning versus shallow learning\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
